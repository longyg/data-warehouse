a1.sources = r1 r2
a1.channels = c1 c2
a1.sinks = k1 k2

a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = bigdata01:9092,bigdata02:9092,bigdata03:9092
a1.sources.r1.kafka.topics = topic_start

a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.batchSize = 5000
a1.sources.r2.batchDurationMillis = 2000
a1.sources.r2.kafka.bootstrap.servers = bigdata01:9092,bigdata02:9092,bigdata03:9092
a1.sources.r2.kafka.topics = topic_event

a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/software/flume/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/software/flume/data/behavior1/
a1.channels.c1.keep-alive = 6

a1.channels.c2.type = file
a1.channels.c2.checkpointDir = /opt/software/flume/checkpoint/behavior2
a1.channels.c2.dataDirs = /opt/software/flume/data/behavior2/
a1.channels.c2.keep-alive = 6

a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = logstart-
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.fileType = DataStream
#a1.sinks.k1.hdfs.codeC = lzop

a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = logevent-
a1.sinks.k2.hdfs.rollInterval = 10
a1.sinks.k2.hdfs.rollSize = 134217728
a1.sinks.k2.hdfs.rollCount = 0
a1.sinks.k2.hdfs.fileType = DataStream
#a1.sinks.k2.hdfs.codeC = lzop

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

a1.sources.r2.channels = c2
a1.sinks.k2.channel = c2